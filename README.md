# Automatic Differentiation with JAX Tutorial

Tutorial Walkthrough:
https://medium.com/@micky.multani/auto-differentials-with-jax-rediscovering-mathematics-in-the-world-of-machine-learning-a7bb50c36439 

## Overview
This repository contains a comprehensive tutorial demonstrating the use of JAX for automatic differentiation in the context of machine learning. The tutorial is designed to bridge the gap between theoretical mathematical concepts learned in classrooms and their practical applications in modern machine learning and AI.

## Contents
JAX_Differentiation_Tutorial.ipynb: A Jupyter Notebook that walks you through the process of using JAX to perform automatic differentiation, set against the backdrop of a classic machine learning problem using the Iris dataset.

## Getting Started
To get started with this tutorial:

Clone the repository:
```
git clone https://github.com/mickymult/JAX-Machinelearning.git
```
Open the JAX_Differentiation_Tutorial.ipynb in Google Colab or Jupyter Notebook.

## Highlights
Introduction to Automatic Differentiation: Understanding the concept of differentiation and its importance in machine learning.
Practical Application: Applying differentiation to train a neural network using the Iris dataset.
Before and After Training Analysis: Evaluating the model's performance before and after training to understand the impact.
Data Visualization: Visualizing the Iris dataset and the model's learning process.

## Requirements
JAX
Flax
Matplotlib
NumPy
Scikit-learn

## Contributions
Contributions to this tutorial are welcome. Feel free to fork the repository and submit pull requests.

## License
This project is open source and available under the MIT License.

## Acknowledgments
A special thanks to all contributors and maintainers of the JAX library for making advanced machine learning techniques more accessible to the wider community.

